{"cells":[{"cell_type":"markdown","metadata":{"id":"ofPup9QCJeoI"},"source":["# Dataset Builder v1 — Boletín Oficial CABA (Train / Test / Val)\n","\n","Genera candidatos (fragmentos) **para etiquetar** a partir de PDFs del Boletín Oficial de CABA.\n","\n","**Características**\n","- Lee todos los PDF de la carpeta de origen en Drive en **orden lexicográfico**.\n","- **Ventana por oraciones**: toma la oración donde aparece el *hit* y **2 oraciones** a cada lado (máx. ~400–600 palabras).\n","- Busca candidatos por **VERBO**, **KEYWORD**, **NORMAR** (configurable). **TEMAS** se usa **solo** para anotar la columna `temas`.\n","- Limpieza mínima (espacios, deduplicación por hash).\n","- **CSV único** (append idempotente) con columnas fijas; `label` queda **vacío** para etiquetar manualmente.\n","- **Mueve** el PDF procesado a la carpeta correspondiente.\n","- **Barra de progreso** (tqdm) y resumen por archivo.\n"],"id":"ofPup9QCJeoI"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SMAx6BziJeoL","executionInfo":{"status":"ok","timestamp":1762469179287,"user_tz":180,"elapsed":31297,"user":{"displayName":"Juan Draghi","userId":"09685788873956471132"}},"outputId":"29f4d1e3-8ed6-485d-ba99-9e74c5b1f0d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Entorno listo ✅\n"]}],"source":["# ===== 1) Dependencias e importaciones =====\n","!pip -q install pypdf nltk tqdm > /dev/null\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, csv, hashlib, shutil\n","from typing import List, Tuple, Set\n","\n","import pandas as pd\n","from pypdf import PdfReader\n","from tqdm import tqdm\n","import re\n","import nltk\n","nltk.download('punkt', quiet=True)\n","try:\n","    nltk.download('punkt_tab', quiet=True)\n","except Exception as _e:\n","    print('Aviso: no se pudo descargar punkt_tab automáticamente:', _e)\n","from nltk.tokenize import sent_tokenize\n","\n","print('Entorno listo ✅')"],"id":"SMAx6BziJeoL"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acvjc3g9JeoM","executionInfo":{"status":"ok","timestamp":1762469180174,"user_tz":180,"elapsed":892,"user":{"displayName":"Juan Draghi","userId":"09685788873956471132"}},"outputId":"fae5ba64-d851-4782-cc93-f0254af11d11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Configuración cargada ✅\n"]}],"source":["# ===== 2) Configuración =====\n","# >>> EDITABLE <<<\n","\n","# Carpetas en Google Drive\n","RAW_DIR = \"/content/drive/MyDrive/IA/Proyectos/Análisis Boletín Oficial/boletin-ml/data/raw/test_2025\"\n","PROCESSED_DIR = \"/content/drive/MyDrive/IA/Proyectos/Análisis Boletín Oficial/boletin-ml/data/processed/boletines_procesados/test\"\n","CSV_PATH = \"/content/drive/MyDrive/IA/Proyectos/Análisis Boletín Oficial/boletin-ml/data/labels/etiquetas_v3etiquetas_test_H2_2025.csv\"\n","\n","# Split fijo para este lote\n","SPLIT_TAG = \"test\"\n","\n","# Ventana por oraciones\n","WINDOW_SENTENCES_LEFT = 2\n","WINDOW_SENTENCES_RIGHT = 2\n","MAX_WORDS_WINDOW = 600  # recortará si supera este máximo\n","MIN_WORDS_WINDOW = 80   # si queda muy corta, intentará expandir si es posible\n","\n","# Listas de patrones (completalas vos)\n","VERBOS_ACCION = [\n","    r\"\\b[Mm]odifica\\b\", r\"\\b[Mm]odifícase\\b\", r\"\\b[Mm]odificar\\b\",\n","    r\"\\b[Dd]eroga\\b\", r\"\\b[Dd]erogar\\b\", r\"\\b[Dd]erógase\\b\",\n","    r\"\\b[Aa]prueba\\b\", r\"\\b[Aa]probar\\b\", r\"\\b[Aa]pruébese\\b\",\n","    r\"\\b[Dd]eja sin efecto\\b\", r\"\\b[Dd]ejar sin efecto\\b\", r\"\\b[Dd]éjase sin efecto\\b\",\n","    r\"\\b[Ss]ustituye\\b\", r\"\\b[Ss]ustituir\\b\", r\"\\b[Ss]ustitúyese\\b\",\n","    r\"\\b[Ee]stablece\\b\", r\"\\b[Ee]stablecer\\b\", r\"\\b[Ee]stablécese\\b\",\n","    r\"\\b[Ff]ija\\b\", r\"\\b[Ff]ijar\\b\", r\"\\b[Ff]íjese\\b\",\n","    r\"\\b[Dd]etermina\\b\", r\"\\b[Dd]etermínase\\b\", r\"\\b[Dd]eterminar\\b\",\n","    r\"\\b[Rr]eglamenta\\b\", r\"\\b[Rr]eglaméntese\\b\", r\"\\b[Rr]eglamentación\\b\",\n","    r\"\\b[Pp]rorroga\\b\", r\"\\b[Pp]rorrógase\\b\", r\"\\b[Pp]rorrogar\\b\"\n","]\n","\n","KEYWORDS = [\n","    # Construcción\n","    \"código de edificación\",\n","    \"reglamentos técnicos\",\n","    \"reglamento técnico\",\n","    \"planos de mensura\",\n","    \"planos de obra\",\n","    \"planos de instalaciones\",\n","    \"obras en contravención\",\n","    \"accesibilidad\",\n","    \"conservación de fachadas\",\n","\n","    # Digestos normativos\n","    \"compendio normativo\",\n","    \"digesto\",\n","\n","    # Escuelas seguras\n","    \"ueresgp\",\n","    \"régimen de escuelas seguras de gestión privada\",\n","\n","    # Espacio público\n","    \"uso del espacio público\",\n","    \"publicidad exterior\",\n","\n","    # Fiscal y tarifario\n","    \"ley tarifaria\",\n","    \"código fiscal\",\n","    \"unidad tarifaria\",\n","    \"derecho para el desarrollo urbano y el hábitat sustentable\",\n","    \"derecho de construcción sustentable\",\n","    \"derecho de construcción\",\n","\n","    # Habilitaciones\n","    \"código de habilitaciones\",\n","    \"autorización de actividad económica\",\n","    \"autorización de actividades económicas\",\n","    \"autorizaciones de actividades económicas\",\n","    \"habilitación\",\n","    \"habilitación económica\",\n","    \"ley marco de regulación de actividades económicas\",\n","\n","    # Impacto ambiental\n","    \"impacto ambiental\",\n","    \"certificado de aptitud ambiental\",\n","\n","    # Sistema de Autoprotección\n","    \"sistema de autoprotección\",\n","    \"sistemas de autoprotección\",\n","\n","    # Urbanismo\n","    \"código urbanístico\",\n","    \"catastro\",\n","    \"área céntrica\",\n","    \"reurbanización\",\n","    \"zonificación\",\n","    \"planeamiento urbano\",\n","    \"normas urbanísticas\",\n","    \"convenios urbanísticos\",\n","    \"convenio urbanístico\"\n","]\n","\n","PATRONES_NORMAR = [\n","    r\"[Oo]rdenanza(?: [Nn]°?)? ?3\\.?442\",\n","    r\"[Oo]rdenanza(?: [Nn]°?)? ?3\\.?421\",\n","    r\"[Dd]isposici[oó]n(?: [Nn]°?)? ?3\\.?500(?:[-/]?GCABA)?[-/]?DGOEP[-/]?16\",\n","    r\"[Dd]isposici[oó]n(?: [Nn]°?)? ?331(?:[-/]?GCABA)?[-/]?DGDCIV[-/]?25\",\n","    r\"[Dd]isposici[oó]n(?: [Nn]°?)? ?89(?:[-/]?GCABA)?[-/]?DGROC[-/]?24\",\n","    r\"[Dd]isposici[oó]n(?: [Nn]°?)? ?526(?:[-/]?GCABA)?[-/]?DGFYCO[-/]?24\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?275(?:[-/]?GCABA)?[-/]?APRA[-/]?23\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?188(?:[-/]?GCABA)?[-/]?SSGU[-/]?24\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?160(?:[-/]?GCABA)?[-/]?SSHA[-/]?24\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?96(?:[-/]?GCABA)?[-/]?AGC[-/]?25\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?345(?:[-/]?GCABA)?[-/]?AGC[-/]?21\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?103(?:[-/]?GCABA)?[-/]?APRA[-/]?25\",\n","    r\"[Rr]esoluci[oó]n(?: [Nn]°?)? ?1(?:[-/]?GCABA)?[-/]?MEPHUGC[-/]?25\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?51/18\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?86/19\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?87/19\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?99/19\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?105/19\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?475/20\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?129/25\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?116/25\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?164/25\",\n","    r\"[Dd]ecreto(?: [Nn]°?)? ?189/25\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?123\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?2\\.?189\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?2\\.?936\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?5\\.?920\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.101\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.437\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.776\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.779\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.099\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.100\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.438\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.508\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.509\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.806\",\n","    r\"[Ll]ey(?: [Nn]°?)? ?6\\.769\"\n","]\n","\n","TEMAS = [\n","  \"construcción\",\n","  \"digestos normativos\",\n","  \"escuelas seguras\",\n","  \"espacio público\",\n","  \"fiscal y tarifario\",\n","  \"habilitaciones\",\n","  \"impacto ambiental\",\n","  \"sistema de autoprotección\",\n","  \"urbanismo\"\n","]\n","\n","# Columnas del CSV (orden fijo)\n","CSV_COLUMNS = [\n","    \"id\",\"split\",\"fecha\",\"anio\",\"boletin_nro\",\"origen_pdf\",\"fuente_url\",\"organismo_emisor\",\"tipo_figura_normativa\",\n","    \"titulo\",\"fragmento\",\"label\",\"anotador\",\"notas\",\"is_ambiguo\",\"deroga_modifica_ref\",\"numero_norma\",\"temas\",\"origen_flags\"\n","]\n","\n","# Crear carpetas si no existen\n","os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n","os.makedirs(PROCESSED_DIR, exist_ok=True)\n","\n","# Aviso útil\n","if len(VERBOS_ACCION)==0 and len(KEYWORDS)==0 and len(PATRONES_NORMAR)==0:\n","    print('⚠️ Advertencia: VERBOS_ACCION, KEYWORDS y PATRONES_NORMAR están vacías. No se generarán fragmentos hasta completarlas.')\n","\n","print('Configuración cargada ✅')"],"id":"acvjc3g9JeoM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cS8a5UBgJeoM","executionInfo":{"status":"ok","timestamp":1762469180203,"user_tz":180,"elapsed":25,"user":{"displayName":"Juan Draghi","userId":"09685788873956471132"}},"outputId":"e218937a-3fb9-444e-a656-59bb7cd84d23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Utilidades listas ✅\n"]}],"source":["# ===== 3) Utilidades =====\n","def read_pdf_text_by_page(pdf_path: str) -> List[str]:\n","    texts = []\n","    try:\n","        reader = PdfReader(pdf_path)\n","        for page in reader.pages:\n","            t = page.extract_text() or \"\"\n","            texts.append(t)\n","    except Exception as e:\n","        print(f\"[ERROR] Leyendo PDF: {pdf_path}: {e}\")\n","        return []\n","    return texts\n","\n","def sent_segment(text: str) -> List[str]:\n","    \"\"\"\n","    Segmenta en oraciones (NLTK español) con fallback regex.\n","    Intenta con 'spanish' (punkt_tab); si falta, descarga y reintenta.\n","    Si falla igual, usa un separador regex simple.\n","    \"\"\"\n","    try:\n","        sents = sent_tokenize(text, language='spanish')\n","    except LookupError:\n","        # Reintenta descargando recursos\n","        try:\n","            nltk.download('punkt', quiet=True)\n","            nltk.download('punkt_tab', quiet=True)\n","            sents = sent_tokenize(text, language='spanish')\n","        except Exception:\n","            # Fallback regex (sin imports dentro de la función)\n","            sents = re.split(r'(?<=[.!?])\\s+(?=[A-ZÁÉÍÓÚÑ])', text)\n","    except Exception:\n","        sents = re.split(r'(?<=[.!?])\\s+(?=[A-ZÁÉÍÓÚÑ])', text)\n","\n","    sents = [re.sub(r\"\\s+\", \" \", s).strip() for s in sents if s and s.strip()]\n","    return sents\n","\n","def normalize_for_hash(s: str) -> str:\n","    s = s.lower()\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def sha1(s: str) -> str:\n","    return hashlib.sha1(s.encode('utf-8')).hexdigest()\n","\n","def any_match(patterns: List[str], text: str) -> bool:\n","    for pat in patterns:\n","        if re.search(pat, text, flags=re.IGNORECASE):\n","            return True\n","    return False\n","\n","def window_by_sentences(sents: List[str], idx: int, k_left=2, k_right=2, max_words=600) -> str:\n","    left = max(0, idx - k_left)\n","    right = min(len(sents), idx + k_right + 1)\n","    window = sents[left:right]\n","    text = \" \".join(window)\n","    words = text.split()\n","    if len(words) > max_words:\n","        while len(words) > max_words and (k_left > 0 or k_right > 0) and (right - left > 1):\n","            if k_right >= k_left:\n","                right -= 1\n","            else:\n","                left += 1\n","            window = sents[left:right]\n","            text = \" \".join(window)\n","            words = text.split()\n","    return text\n","\n","def try_extract_fecha_from_filename(name: str) -> Tuple[str, str]:\n","    name = name.replace(\" \", \"_\")\n","    m = re.search(r\"(20\\d{2})[-_](\\d{2})[-_](\\d{2})\", name)\n","    if m:\n","        y, mo, d = m.group(1), m.group(2), m.group(3)\n","        return f\"{y}-{mo}-{d}\", y\n","    m = re.search(r\"(\\d{2})[-_](\\d{2})[-_](20\\d{2})\", name)\n","    if m:\n","        d, mo, y = m.group(1), m.group(2), m.group(3)\n","        return f\"{y}-{mo}-{d}\", y\n","    m = re.search(r\"(20\\d{2})(\\d{2})(\\d{2})\", name)\n","    if m:\n","        y, mo, d = m.group(1), m.group(2), m.group(3)\n","        return f\"{y}-{mo}-{d}\", y\n","    m = re.search(r\"(20\\d{2})\", name)\n","    if m:\n","        y = m.group(1)\n","        return \"\", y\n","    return \"\", \"\"\n","\n","def extract_numero_norma(text: str) -> str:\n","    m = re.search(r\"\\b(ley|decreto|resoluci(ó|o)n|disposici(ó|o)n|ordenanza)\\b\\s*(n[º°o]?\\s*\\d+[./-]?\\d*)\", text, re.IGNORECASE)\n","    if m:\n","        return m.group(0)\n","    m = re.search(r\"\\bN[º°o]\\s*\\d+[./-]?\\d*\\b\", text, re.IGNORECASE)\n","    if m:\n","        return m.group(0)\n","    return \"\"\n","\n","def extract_temas(text: str) -> str:\n","    # Usa las expresiones de TEMAS para anotar, no para disparar\n","    found = []\n","    for pat in TEMAS:\n","        hits = re.findall(pat, text, flags=re.IGNORECASE)\n","        if hits:\n","            found.append(pat)\n","    seen = set()\n","    uniq = []\n","    for f in found:\n","        if f not in seen:\n","            seen.add(f)\n","            uniq.append(f)\n","    return \";\".join(uniq)\n","\n","def has_deroga_modifica(text: str) -> str:\n","    if re.search(r\"\\b(deroga(n|r|do|da|das|dos)?|modifica(n|r|do|da|das|dos)?|sustituye(n|r|do|da|das|dos)?)\\b\", text, re.IGNORECASE):\n","        return \"1\"\n","    return \"0\"\n","\n","def ensure_csv(csv_path: str, columns: List[str]):\n","    if not os.path.exists(csv_path):\n","        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","            w = csv.writer(f)\n","            w.writerow(columns)\n","\n","def load_existing_keys(csv_path: str) -> Set[str]:\n","    keys = set()\n","    if os.path.exists(csv_path):\n","        try:\n","            for chunk in pd.read_csv(csv_path, chunksize=5000, dtype=str, keep_default_na=False):\n","                for _, row in chunk.iterrows():\n","                    frag = (row.get(\"fragmento\", \"\") or \"\")\n","                    fecha = (row.get(\"fecha\", \"\") or \"\")\n","                    origen = (row.get(\"origen_pdf\", \"\") or \"\")\n","                    key = sha1(normalize_for_hash(frag) + \"|\" + fecha + \"|\" + origen)\n","                    keys.add(key)\n","        except Exception as e:\n","            print(f\"[WARN] No se pudieron cargar todas las llaves existentes: {e}\")\n","    return keys\n","\n","def append_rows_csv(csv_path: str, rows: List[List[str]]):\n","    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n","        w = csv.writer(f)\n","        for r in rows:\n","            w.writerow(r)\n","\n","print('Utilidades listas ✅')"],"id":"cS8a5UBgJeoM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8uomGE4JeoN","executionInfo":{"status":"ok","timestamp":1762469180255,"user_tz":180,"elapsed":47,"user":{"displayName":"Juan Draghi","userId":"09685788873956471132"}},"outputId":"a0548371-83b7-4e53-d5e2-14f31635972c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline listo ✅\n"]}],"source":["# ===== 4) Pipeline principal =====\n","def process_pdf(pdf_path: str, existing_keys: Set[str]) -> Tuple[int, int]:\n","    \"\"\"\n","    Procesa un PDF y devuelve (filas_nuevas_agregadas, descartadas_locales).\n","    - Gating sobre la ORACIÓN (sent):\n","        Aceptar si:\n","          1) contiene KEYWORDS, o\n","          2) contiene PATRONES_NORMAR, o\n","          3) VERBOS_ACCION y PATRONES_NORMAR, o\n","          4) KEYWORDS y PATRONES_NORMAR\n","        (Nunca aceptar VERBOS_ACCION solo)\n","    - Agrupa oraciones disparadoras contiguas en un solo segmento -> una ventana por segmento.\n","    - Evita ventanas solapadas fuertes (IoU > 0.6) en la misma página.\n","    - Filtro final por VENTANA: si quedó solo VERBO, descarta.\n","    \"\"\"\n","    base = os.path.basename(pdf_path)\n","    fecha_iso, anio = try_extract_fecha_from_filename(base)\n","\n","    pages = read_pdf_text_by_page(pdf_path)\n","    if not pages:\n","        # mover el PDF aunque no se pudo leer, para que no se repita\n","        try:\n","            dest = os.path.join(PROCESSED_DIR, base)\n","            if os.path.exists(dest):\n","                root, ext = os.path.splitext(base)\n","                dest = os.path.join(PROCESSED_DIR, f\"{root}__moved{ext}\")\n","            shutil.move(pdf_path, dest)\n","        except Exception as e:\n","            print(f\"[WARN] No se pudo mover {base} a processed: {e}\")\n","        return 0, 0\n","\n","    new_rows = []\n","    discarded_local = 0\n","\n","    # dedup dentro del PDF por contenido normalizado\n","    seen_hashes_local = set()\n","\n","    for p_idx, page_text in enumerate(pages):\n","        if not page_text or not page_text.strip():\n","            continue\n","\n","        sents = sent_segment(page_text)\n","        if not sents:\n","            continue\n","\n","        # ---------- 1) GATING por ORACIÓN: recolectar índices disparadores ----------\n","        triggers_idx = []\n","        for i, sent in enumerate(sents):\n","            has_verbo   = any_match(VERBOS_ACCION, sent)\n","            has_kw      = any_match(KEYWORDS, sent)\n","            has_normar  = any_match(PATRONES_NORMAR, sent)\n","\n","            has_candidate = (\n","                has_kw\n","                or has_normar\n","                or (has_verbo and has_normar)\n","                or (has_kw and has_normar)\n","            )\n","            # Blindaje extra: nunca verbo solo\n","            if has_verbo and not (has_kw or has_normar):\n","                has_candidate = False\n","\n","            if has_candidate:\n","                triggers_idx.append(i)\n","\n","        if not triggers_idx:\n","            continue\n","\n","        # ---------- 2) AGRUPAR ÍNDICES CONTIGUOS EN SEGMENTOS ----------\n","        # Permite una \"grieta\" de hasta 1 oración entre disparos para agrupar\n","        GAP = 1\n","        segments = []\n","        start = prev = triggers_idx[0]\n","        for idx in triggers_idx[1:]:\n","            if idx - prev <= GAP:\n","                prev = idx\n","            else:\n","                segments.append((start, prev))\n","                start = prev = idx\n","        segments.append((start, prev))  # último segmento\n","\n","        # Para evitar ventanas muy solapadas en la misma página\n","        spans_seen_page: List[Tuple[int,int]] = []\n","\n","        # Utilidad local: IoU de rangos de oraciones [L,R]\n","        def iou(span_a: Tuple[int,int], span_b: Tuple[int,int]) -> float:\n","            aL, aR = span_a\n","            bL, bR = span_b\n","            inter = max(0, min(aR, bR) - max(aL, bL) + 1)\n","            union = (aR - aL + 1) + (bR - bL + 1) - inter\n","            return inter / union if union else 0.0\n","\n","        for (seg_start, seg_end) in segments:\n","            # ---------- 3) Construir ventana por oraciones ----------\n","            left  = max(0, seg_start - WINDOW_SENTENCES_LEFT)\n","            right = min(len(sents) - 1, seg_end + WINDOW_SENTENCES_RIGHT)\n","\n","            # Evitar ventanas fuertemente solapadas con ya aceptadas (IoU > 0.6)\n","            if any(iou((left, right), span) > 0.6 for span in spans_seen_page):\n","                discarded_local += 1\n","                continue\n","            spans_seen_page.append((left, right))\n","\n","            # Construir texto de la ventana y recortar por MAX_WORDS_WINDOW si hace falta\n","            window_sents = sents[left:right+1]\n","            window = \" \".join(window_sents)\n","            words = window.split()\n","            if len(words) > MAX_WORDS_WINDOW:\n","                # recorta simétrico manteniendo oraciones completas\n","                L, R = left, right\n","                while len(words) > MAX_WORDS_WINDOW and (R - L) > 0:\n","                    # recorta del lado más largo respecto al centro del segmento\n","                    if (R - seg_end) >= (seg_start - L):\n","                        R -= 1\n","                    else:\n","                        L += 1\n","                    window_sents = sents[L:R+1]\n","                    window = \" \".join(window_sents)\n","                    words = window.split()\n","                left, right = L, R  # actualizar por consistencia\n","\n","            # Si quedó demasiado corta, intenta expandir un paso si es posible\n","            if len(words) < MIN_WORDS_WINDOW:\n","                L = max(0, left - 1)\n","                R = min(len(sents) - 1, right + 1)\n","                window = \" \".join(sents[L:R+1])\n","                left, right = L, R\n","\n","            # ---------- 4) Flags por VENTANA y filtro \"solo VERBO\" ----------\n","            flags = []\n","            if any_match(VERBOS_ACCION, window):     flags.append(\"VERBO\")\n","            if any_match(KEYWORDS, window):          flags.append(\"KEYWORD\")\n","            if any_match(PATRONES_NORMAR, window):   flags.append(\"NORMAR\")\n","\n","            # Si en la ventana quedó solo VERBO, descartar\n","            if (\"VERBO\" in flags) and (\"KEYWORD\" not in flags) and (\"NORMAR\" not in flags):\n","                discarded_local += 1\n","                continue\n","\n","            # ---------- 5) Deduplicaciones ----------\n","            window_norm = normalize_for_hash(window)\n","            h = sha1(window_norm)\n","            if h in seen_hashes_local:\n","                discarded_local += 1\n","                continue\n","            seen_hashes_local.add(h)\n","\n","            uniq_key = sha1(window_norm + \"|\" + (fecha_iso or \"\") + \"|\" + base)\n","            if uniq_key in existing_keys:\n","                discarded_local += 1\n","                continue\n","\n","            # ---------- 6) Extraer metadatos y armar fila ----------\n","            numero_norma = extract_numero_norma(window)\n","            temas_detect = extract_temas(window)  # TEMAS solo anota, no dispara\n","            deroga_modifica_ref = has_deroga_modifica(window)\n","            origen_flags = \";\".join(flags)\n","\n","            # ID estable: fecha + página + span de oraciones + hash corto\n","            id_str = f\"{(fecha_iso or 'NA').replace('-', '')}-p{p_idx:03d}-s{left:03d}-e{right:03d}-h{h[:8]}\"\n","\n","            row = [\n","                id_str,          # id\n","                \"train\",         # split (ajusta si estás corriendo val/test)\n","                fecha_iso,       # fecha\n","                anio,            # anio\n","                \"\",              # boletin_nro\n","                base,            # origen_pdf\n","                \"\",              # fuente_url\n","                \"\",              # organismo_emisor\n","                \"\",              # tipo_figura_normativa\n","                \"\",              # titulo\n","                window,          # fragmento\n","                \"\",              # label (vacío)\n","                \"\",              # anotador\n","                \"\",              # notas\n","                \"0\",             # is_ambiguo\n","                deroga_modifica_ref,  # deroga_modifica_ref\n","                numero_norma,    # numero_norma\n","                temas_detect,    # temas\n","                origen_flags,    # origen_flags\n","            ]\n","            new_rows.append(row)\n","\n","    # ---------- 7) Append CSV y mover PDF ----------\n","    appended = 0\n","    if new_rows:\n","        append_rows_csv(CSV_PATH, new_rows)\n","        # actualizar llaves globales para evitar reinsertar si re-corremos\n","        for r in new_rows:\n","            frag  = r[CSV_COLUMNS.index(\"fragmento\")]\n","            fecha = r[CSV_COLUMNS.index(\"fecha\")] or \"\"\n","            origen = r[CSV_COLUMNS.index(\"origen_pdf\")] or \"\"\n","            key = sha1(normalize_for_hash(frag) + \"|\" + fecha + \"|\" + origen)\n","            existing_keys.add(key)\n","        appended = len(new_rows)\n","\n","    try:\n","        dest = os.path.join(PROCESSED_DIR, base)\n","        if os.path.exists(dest):\n","            root, ext = os.path.splitext(base)\n","            dest = os.path.join(PROCESSED_DIR, f\"{root}__moved{ext}\")\n","        shutil.move(pdf_path, dest)\n","    except Exception as e:\n","        print(f\"[WARN] No se pudo mover {base} a processed: {e}\")\n","\n","    return appended, discarded_local\n","\n","\n","def run_batch(limit_pdfs=None):\n","    ensure_csv(CSV_PATH, CSV_COLUMNS)\n","    existing = load_existing_keys(CSV_PATH)\n","\n","    pdfs = [os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.lower().endswith('.pdf')]\n","    pdfs.sort()\n","\n","    total_new = 0\n","    total_discard = 0\n","\n","    if not pdfs:\n","        print(\"No hay PDFs para procesar en:\", RAW_DIR)\n","        return\n","\n","    for idx, pdf in enumerate(tqdm(pdfs, desc=\"Procesando PDFs\", unit=\"pdf\")):\n","        if limit_pdfs is not None and idx >= limit_pdfs:\n","            tqdm.write(f\"Corte manual: solo {limit_pdfs} PDFs.\")\n","            break\n","        base = os.path.basename(pdf)\n","        new_rows, disc = process_pdf(pdf, existing)\n","        total_new += new_rows\n","        total_discard += disc\n","        tqdm.write(f\"Analizado: {base} | filas nuevas: {new_rows}\")\n","\n","    print(\"\\nResumen\")\n","    print(\"PDFs procesados:\", min(len(pdfs), limit_pdfs) if limit_pdfs else len(pdfs))\n","    print(\"Filas nuevas agregadas:\", total_new)\n","    print(\"Descartadas por duplicado (local):\", total_discard)\n","    print(\"CSV:\", CSV_PATH)\n","    print(\"Processed dir:\", PROCESSED_DIR)\n","\n","print('Pipeline listo ✅')"],"id":"n8uomGE4JeoN"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33ptIakOJeoN","executionInfo":{"status":"ok","timestamp":1762470189810,"user_tz":180,"elapsed":370,"user":{"displayName":"Juan Draghi","userId":"09685788873956471132"}},"outputId":"95baeeae-b695-43b0-ca1d-ebe28ebb9196"},"outputs":[{"output_type":"stream","name":"stdout","text":["No hay PDFs para procesar en: /content/drive/MyDrive/IA/Proyectos/Análisis Boletín Oficial/boletin-ml/data/raw/test_2025\n"]}],"source":["# ===== 5) Ejecutar =====\n","run_batch(limit_pdfs=8)"],"id":"33ptIakOJeoN"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
